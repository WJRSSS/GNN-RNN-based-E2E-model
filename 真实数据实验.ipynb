{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c03ca0-1597-446b-9d3b-94ff8d546aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Encoder\n",
    "import Decoder\n",
    "import GNN_LSTM\n",
    "from MQRNN import MQRNN\n",
    "from data import MQRNN_dataset, read_df, read_df2, read_df3,loss_result\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data import MQRNN_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import random\n",
    "from train import train\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from GNN_func import build_and_solve_milp, parse_mps_and_build_graphs\n",
    "#from NEW_GNN0421 import build_graph_from_mps_sol\n",
    "from Student_model import *\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea18a096-51e0-4b73-9539-267a05e943d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from gurobipy import read\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.nn as geom_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369c5553-895d-41c7-bb0d-ddd33a343c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 参数设置 ======================\n",
    "price_data = pd.read_excel('./32SKU/price_32.xlsx')\n",
    "target_width = price_data.shape[0]\n",
    "SKUS = []\n",
    "for i in range(target_width):\n",
    "    SKUS.append(price_data.iloc[i][0])\n",
    "prices = {}\n",
    "for i in range(target_width):\n",
    "    prices[SKUS[i]] = price_data.iloc[i][1]\n",
    "# SKUS = ['EAEZAIN9501', 'EAEZAIN9501EZ3', 'NAEZAIN9501',\n",
    "#         'NAEZAIN9501EZ', 'NAEZAIN9501EZ2', 'NAEZAIN9501EZ3',\n",
    "#         'NAEZAIN9501EZ4', 'NAEZAIN9502']\n",
    "\n",
    "sku_index = {sku: i for i, sku in enumerate(SKUS)}\n",
    "TOTAL_PERIODS = 99\n",
    "window_size = 7\n",
    "num_windows = TOTAL_PERIODS - window_size + 1  # 滑动窗口的数量\n",
    "TRAIN_RATIO = 0.8  # 训练集80%\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 加载Excel数据\n",
    "# df_demand = pd.read_excel('demand_0206.xlsx')\n",
    "# df_supply = pd.read_excel('supply_0206.xlsx')\n",
    "# df_inventory = pd.read_excel('inventory_0206.xlsx')\n",
    "\n",
    "#32个sku\n",
    "df_demand = pd.read_excel('./32SKU/demand_top32.xlsx')\n",
    "df_supply = pd.read_excel('./32SKU/supply_top32.xlsx')\n",
    "df_inventory = pd.read_excel('./32SKU/inventory_top32.xlsx')\n",
    "\n",
    "# 供给量\n",
    "supply = df_supply.set_index('ID').to_dict(orient='index')\n",
    "# 销售量\n",
    "demand = df_demand.set_index('ID').to_dict(orient='index')\n",
    "# 库存日报\n",
    "inventory = df_inventory.set_index('ID').to_dict(orient='index')\n",
    "\n",
    "# 成本参数设置\n",
    "# prices = {\n",
    "#      'EAEZAIN9501': 65, 'EAEZAIN9501EZ3': 70, 'NAEZAIN9501': 69,\n",
    "#      'NAEZAIN9501EZ': 84, 'NAEZAIN9501EZ2': 89, 'NAEZAIN9501EZ3': 74,\n",
    "#      'NAEZAIN9501EZ4': 79, 'NAEZAIN9502': 74\n",
    "# }\n",
    "\n",
    "# 计算各项成本\n",
    "trans_cost = {}\n",
    "for s1 in SKUS:\n",
    "    for s2 in SKUS:\n",
    "        if s1 != s2:\n",
    "            # 改造成本 = 价格差的20%/6\n",
    "            trans_cost[(s1, s2)] = 1.5\n",
    "\n",
    "holding_cost_factor = 0.068\n",
    "shortage_cost_factor = 0.3\n",
    "M = 1e5\n",
    "# 库存持有成本和缺货成本\n",
    "holding_cost = {s: prices[s] * holding_cost_factor for s in SKUS}\n",
    "shortage_cost = {s: prices[s] * shortage_cost_factor for s in SKUS}\n",
    "# 输出目录准备\n",
    "mps_dir = \"mps_files_32SKU\"\n",
    "os.makedirs(mps_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca04929b-8205-498b-83e3-d41a3ba8b8c8",
   "metadata": {},
   "source": [
    "# ====================== 1. MILP建模与求解 ======================\n",
    "for w in range(num_windows):\n",
    "    print(f\"正在处理周期 {w+1}/{num_windows}...\")\n",
    "\n",
    "    model = gp.Model(f\"SKU_Transformation_Week_{w}\")\n",
    "    start_day = w\n",
    "    end_day = w + window_size\n",
    "\n",
    "    # 初始库存为前一天的库存\n",
    "    if start_day == 0:\n",
    "        current_inv = {s: df_inventory.loc[0, s] for s in SKUS}\n",
    "    else:\n",
    "        current_inv = {s: df_inventory.loc[start_day - 1, s] for s in SKUS}\n",
    "\n",
    "    # ----------------------------\n",
    "    # 定义变量\n",
    "    # ----------------------------\n",
    "    weekly_trans_vars = model.addVars(\n",
    "        [(s1, s2) for s1 in SKUS for s2 in SKUS if s1 != s2 ],\n",
    "        name=\"Weekly_Trans\", vtype=GRB.INTEGER)\n",
    "    for (s1, s2), var in weekly_trans_vars.items():\n",
    "        var.VarName = f\"Weekly_Trans_{s1}_{s2}_w{w}\"\n",
    "    inv_vars = model.addVars(SKUS, name=\"Inv\", vtype=GRB.INTEGER ,lb=0)\n",
    "    short_vars = model.addVars(SKUS, name=\"Short\", vtype=GRB.INTEGER ,lb=0)\n",
    "    # delta_vars = model.addVars(SKUS, name=\"IsShort\", vtype=GRB.BINARY)\n",
    "    # ----------------------------\n",
    "    # 约束1：库存平衡\n",
    "    # ----------------------------\n",
    "    weekly_supply = {s: 0 for s in SKUS}\n",
    "    weekly_demand = {s: 0 for s in SKUS}\n",
    "    for d in range(window_size):\n",
    "        day = start_day + d\n",
    "        idx = w * window_size + d\n",
    "        for s in SKUS:\n",
    "            weekly_supply[s] += df_supply.loc[day, s]\n",
    "            weekly_demand[s] += df_demand.loc[day, s]\n",
    "\n",
    "    for s in SKUS:\n",
    "        trans_in = gp.quicksum(weekly_trans_vars[s2, s] for s2 in SKUS if s2 != s)\n",
    "        trans_out = gp.quicksum(weekly_trans_vars[s, s2] for s2 in SKUS if s2 != s)\n",
    "\n",
    "        # TS_s= (current_inv[s] + weekly_supply[s] + trans_in - trans_out)\n",
    "        # # 库存下限\n",
    "        # model.addConstr(inv_vars[s] >= TS_s - weekly_demand[s])\n",
    "        # model.addConstr(inv_vars[s] <= (TS_s - weekly_demand[s]) + M * (1 - delta_vars[s]))\n",
    "        # model.addConstr(short_vars[s] >= weekly_demand[s] - TS_s)\n",
    "        # model.addConstr(short_vars[s] <= (weekly_demand[s] - TS_s) + M * delta_vars[s])\n",
    "\n",
    "        model.addConstr(\n",
    "            current_inv[s] + weekly_supply[s] + trans_in + short_vars[s] ==\n",
    "            weekly_demand[s] + inv_vars[s] + trans_out,\n",
    "            name=f\"WeeklyBalance_{s}_{w}\"\n",
    "        )\n",
    "    # ----------------------------\n",
    "    # 约束2：缺货不能为负\n",
    "    # ----------------------------\n",
    "    model.addConstrs((short_vars[s] >= 0 for s in SKUS), name=\"Shortage_NonNegative\")\n",
    "    # ----------------------------\n",
    "    # 约束3：高配不能转低配\n",
    "    # ----------------------------\n",
    "    for s1 in SKUS:\n",
    "        for s2 in SKUS:\n",
    "            if sku_index[s1] > sku_index[s2]:\n",
    "                model.addConstr(weekly_trans_vars[s1, s2] == 0, name=f\"No_Downgrade_{s1}_{s2}_{w}\")\n",
    "    # ----------------------------\n",
    "    # 目标函数\n",
    "    # ----------------------------\n",
    "    revenue = gp.quicksum(\n",
    "        prices[s] * (weekly_demand[s] - short_vars[s]) for s in SKUS\n",
    "    )\n",
    "    cost = (\n",
    "        gp.quicksum(weekly_trans_vars[s1, s2] * trans_cost[s1, s2]\n",
    "                    for s1 in SKUS for s2 in SKUS if s1 != s2) +\n",
    "        gp.quicksum(inv_vars[s] * holding_cost[s] for s in SKUS ) +\n",
    "        gp.quicksum(short_vars[s] * shortage_cost[s] for s in SKUS )\n",
    "    )\n",
    "    model.setObjective(revenue - cost, GRB.MAXIMIZE)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 模型优化与导出\n",
    "    # ----------------------------\n",
    "    model.setParam('OutputFlag', 0)\n",
    "    model.write(f\"{mps_dir}/MILP_week_{w}.mps\")\n",
    "    model.optimize()\n",
    "    model.write(f\"{mps_dir}/MILP_week_{w}.sol\")  # 保存解文件\n",
    "\n",
    "    print(f\"MILP solved successfully. Obj: {model.ObjVal:.2f}\")\n",
    "    # 获取所有变量值\n",
    "    var_dict = {v.VarName: v.X for v in model.getVars()}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "MPS_DIR = f\"{mps_dir}/MILP_week_{w}.mps\"\n",
    "SOL_DIR = f\"{mps_dir}/MILP_week_{w}.sol\"\n",
    "\n",
    "# 2. 解析 MPS 文件函数\n",
    "def parse_mps_file(file_path):\n",
    "    model = read(MPS_DIR)\n",
    "    variables = [var.VarName for var in model.getVars()]\n",
    "    constraints = [con.ConstrName for con in model.getConstrs()]\n",
    "    edges = []\n",
    "    for i, var in enumerate(variables):\n",
    "        for j, con in enumerate(constraints):\n",
    "            coef = np.random.choice([0, 1], p=[0.7, 0.3]) * np.random.uniform(0.1, 1.0)\n",
    "            if coef != 0:\n",
    "                edges.append((i, j, coef))\n",
    "    return variables, constraints, edges\n",
    "\n",
    "# 3. 解析 SOL 文件函数\n",
    "def parse_sol_file(file_path,week_idx):\n",
    "    variable_values = {}\n",
    "    obj_value = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # 获取目标函数值\n",
    "            if line.startswith(\"# Objective value\"):\n",
    "                obj_value = float(line.split('=')[1].strip())\n",
    "\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            var_name, value_str = parts\n",
    "            try:\n",
    "                value = float(value_str)\n",
    "                variable_values[var_name] = value\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return variable_values, obj_value\n",
    "\n",
    "# 4. 构建 PyG 图结构\n",
    "def safe_normalize_graph_index(x, edge_index):\n",
    "    nodes_used = torch.unique(edge_index)\n",
    "    id_map = {old_id.item(): new_id for new_id, old_id in enumerate(nodes_used)}\n",
    "    new_edge_index = torch.stack([\n",
    "        torch.tensor([id_map[i.item()] for i in edge_index[0]]),\n",
    "        torch.tensor([id_map[i.item()] for i in edge_index[1]])\n",
    "    ], dim=0)\n",
    "    new_x = x[nodes_used]\n",
    "\n",
    "    return new_x, new_edge_index\n",
    "\n",
    "def build_graph_from_mps_sol(mps_path, sol_path,period_id):\n",
    "    variables, constraints, edges = parse_mps_file(mps_path)\n",
    "    varname_to_idx = {var: idx for idx, var in enumerate(variables)}\n",
    "    values, obj = parse_sol_file(sol_path, period_id)\n",
    "    # print(f\"[DEBUG] Week {period_id} parsed {len(values)} variables, example: {list(values.items())[:5]}\")\n",
    "\n",
    "    num_vars = len(variables)\n",
    "    num_cons = len(constraints)\n",
    "    num_nodes = num_vars + num_cons\n",
    "\n",
    "    edge_index = [[], []]\n",
    "    edge_attr = []\n",
    "\n",
    "    for var_idx, con_idx, coef in edges:\n",
    "        edge_index[0].append(var_idx)\n",
    "        edge_index[1].append(num_vars + con_idx)\n",
    "        edge_attr.append([coef])\n",
    "        edge_index[0].append(num_vars + con_idx)\n",
    "        edge_index[1].append(var_idx)\n",
    "        edge_attr.append([coef])\n",
    "\n",
    "    x = torch.eye(num_nodes, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    y_values = [0.0] * num_vars\n",
    "\n",
    "    # 获取SKU——pairs\n",
    "    for var_name, value in values.items():\n",
    "        if var_name in varname_to_idx:\n",
    "            idx = varname_to_idx[var_name]\n",
    "            y_values[idx] = value\n",
    "    y_tensor = torch.tensor(y_values, dtype=torch.float)\n",
    "\n",
    "    var_names = variables\n",
    "    var_names += [''] * num_cons\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y_tensor,\n",
    "    )\n",
    "    data.period_id = torch.tensor([period_id], dtype=torch.long)\n",
    "    data.milp_obj = obj\n",
    "    data.var_names = var_names\n",
    "    return data\n",
    "\n",
    "# for test （可删）\n",
    "# for w in range(num_windows):\n",
    "#     MPS_DIR = f\"{mps_dir}/MILP_week_{w}.mps\"\n",
    "#     SOL_DIR = f\"{mps_dir}/MILP_week_{w}.sol\"\n",
    "#     print(build_graph_from_mps_sol(MPS_DIR, SOL_DIR,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a21871-4dcb-4e27-8ab4-66d2e229102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 解析 MPS 文件函数\n",
    "def parse_mps_file(file_path):\n",
    "    model = read(MPS_DIR)\n",
    "    variables = [var.VarName for var in model.getVars()]\n",
    "    constraints = [con.ConstrName for con in model.getConstrs()]\n",
    "    edges = []\n",
    "    for i, var in enumerate(variables):\n",
    "        for j, con in enumerate(constraints):\n",
    "            coef = np.random.choice([0, 1], p=[0.7, 0.3]) * np.random.uniform(0.1, 1.0)\n",
    "            if coef != 0:\n",
    "                edges.append((i, j, coef))\n",
    "    return variables, constraints, edges\n",
    "\n",
    "\n",
    "# 3. 解析 SOL 文件函数\n",
    "def parse_sol_file(file_path,week_idx):\n",
    "    variable_values = {}\n",
    "    obj_value = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # 获取目标函数值\n",
    "            if line.startswith(\"# Objective value\"):\n",
    "                obj_value = float(line.split('=')[1].strip())\n",
    "\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            var_name, value_str = parts\n",
    "            try:\n",
    "                value = float(value_str)\n",
    "                variable_values[var_name] = value\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    return variable_values, obj_value\n",
    "\n",
    "\n",
    "def build_graph_from_mps_sol(mps_path, sol_path,period_id):\n",
    "    variables, constraints, edges = parse_mps_file(mps_path)\n",
    "    varname_to_idx = {var: idx for idx, var in enumerate(variables)}\n",
    "    values, obj = parse_sol_file(sol_path, period_id)\n",
    "    # print(f\"[DEBUG] Week {period_id} parsed {len(values)} variables, example: {list(values.items())[:5]}\")\n",
    "\n",
    "    num_vars = len(variables)\n",
    "    num_cons = len(constraints)\n",
    "    num_nodes = num_vars + num_cons\n",
    "\n",
    "    edge_index = [[], []]\n",
    "    edge_attr = []\n",
    "\n",
    "    for var_idx, con_idx, coef in edges:\n",
    "        edge_index[0].append(var_idx)\n",
    "        edge_index[1].append(num_vars + con_idx)\n",
    "        edge_attr.append([coef])\n",
    "        edge_index[0].append(num_vars + con_idx)\n",
    "        edge_index[1].append(var_idx)\n",
    "        edge_attr.append([coef])\n",
    "\n",
    "    x = torch.eye(num_nodes, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    y_values = [0.0] * num_vars\n",
    "\n",
    "    # 获取SKU——pairs\n",
    "    for var_name, value in values.items():\n",
    "        if var_name in varname_to_idx:\n",
    "            idx = varname_to_idx[var_name]\n",
    "            y_values[idx] = value\n",
    "    y_tensor = torch.tensor(y_values, dtype=torch.float)\n",
    "\n",
    "    var_names = variables\n",
    "    var_names += [''] * num_cons\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y_tensor,\n",
    "    )\n",
    "    data.period_id = torch.tensor([period_id], dtype=torch.long)\n",
    "    data.milp_obj = obj\n",
    "    data.var_names = var_names\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237da37b-9c5d-4add-a997-2c3f3d006849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从缓存加载数据...\n"
     ]
    }
   ],
   "source": [
    "# 5. 构建数据集\n",
    "data_list = []\n",
    "\n",
    "# 定义缓存文件路径\n",
    "CACHE_FILE = f\"{mps_dir}/data_list_cache.pkl\"\n",
    "\n",
    "# 检查缓存是否存在\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    print(\"从缓存加载数据...\")\n",
    "    with open(CACHE_FILE, 'rb') as f:\n",
    "        data_list = pickle.load(f)\n",
    "else:\n",
    "    for i in range(93):\n",
    "        MPS_DIR = f\"{mps_dir}/MILP_week_{i}.mps\"\n",
    "        SOL_DIR = f\"{mps_dir}/MILP_week_{i}.sol\"\n",
    "        graph_data = build_graph_from_mps_sol(MPS_DIR, SOL_DIR,period_id=i)\n",
    "        data_list.append(graph_data)\n",
    "\n",
    "        with open(CACHE_FILE, 'wb') as f:\n",
    "                pickle.dump(data_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c545cd54-cbc5-41f1-bd56-1a7f1754f572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[1616, 1616], edge_index=[2, 354550], edge_attr=[354550, 1], y=[1056], period_id=[1], milp_obj=63219.87199999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354008], edge_attr=[354008, 1], y=[1056], period_id=[1], milp_obj=95376.532, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354350], edge_attr=[354350, 1], y=[1056], period_id=[1], milp_obj=124994.224, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355462], edge_attr=[355462, 1], y=[1056], period_id=[1], milp_obj=137750.124, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354984], edge_attr=[354984, 1], y=[1056], period_id=[1], milp_obj=157916.772, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354470], edge_attr=[354470, 1], y=[1056], period_id=[1], milp_obj=173612.596, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355910], edge_attr=[355910, 1], y=[1056], period_id=[1], milp_obj=150065.152, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355512], edge_attr=[355512, 1], y=[1056], period_id=[1], milp_obj=152684.09999999998, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353604], edge_attr=[353604, 1], y=[1056], period_id=[1], milp_obj=153611.932, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354790], edge_attr=[354790, 1], y=[1056], period_id=[1], milp_obj=155731.31199999998, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354200], edge_attr=[354200, 1], y=[1056], period_id=[1], milp_obj=159376.908, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353888], edge_attr=[353888, 1], y=[1056], period_id=[1], milp_obj=139367.572, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354398], edge_attr=[354398, 1], y=[1056], period_id=[1], milp_obj=136059.484, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355794], edge_attr=[355794, 1], y=[1056], period_id=[1], milp_obj=117064.69600000001, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354226], edge_attr=[354226, 1], y=[1056], period_id=[1], milp_obj=106406.23199999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354372], edge_attr=[354372, 1], y=[1056], period_id=[1], milp_obj=104818.224, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355364], edge_attr=[355364, 1], y=[1056], period_id=[1], milp_obj=100475.17599999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353254], edge_attr=[353254, 1], y=[1056], period_id=[1], milp_obj=95941.95199999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355514], edge_attr=[355514, 1], y=[1056], period_id=[1], milp_obj=105739.39599999998, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354910], edge_attr=[354910, 1], y=[1056], period_id=[1], milp_obj=106944.972, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355958], edge_attr=[355958, 1], y=[1056], period_id=[1], milp_obj=104642.14, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354058], edge_attr=[354058, 1], y=[1056], period_id=[1], milp_obj=95645.284, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 352886], edge_attr=[352886, 1], y=[1056], period_id=[1], milp_obj=93985.86, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355346], edge_attr=[355346, 1], y=[1056], period_id=[1], milp_obj=96961.992, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353558], edge_attr=[353558, 1], y=[1056], period_id=[1], milp_obj=106153.492, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353956], edge_attr=[353956, 1], y=[1056], period_id=[1], milp_obj=112710.716, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354188], edge_attr=[354188, 1], y=[1056], period_id=[1], milp_obj=98205.15999999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355324], edge_attr=[355324, 1], y=[1056], period_id=[1], milp_obj=98412.38799999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354776], edge_attr=[354776, 1], y=[1056], period_id=[1], milp_obj=90346.116, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354992], edge_attr=[354992, 1], y=[1056], period_id=[1], milp_obj=92939.31599999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354798], edge_attr=[354798, 1], y=[1056], period_id=[1], milp_obj=89652.01199999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354990], edge_attr=[354990, 1], y=[1056], period_id=[1], milp_obj=78689.988, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 356060], edge_attr=[356060, 1], y=[1056], period_id=[1], milp_obj=87731.212, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354714], edge_attr=[354714, 1], y=[1056], period_id=[1], milp_obj=83110.124, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355084], edge_attr=[355084, 1], y=[1056], period_id=[1], milp_obj=66116.456, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354656], edge_attr=[354656, 1], y=[1056], period_id=[1], milp_obj=62393.25199999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354890], edge_attr=[354890, 1], y=[1056], period_id=[1], milp_obj=78492.916, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354128], edge_attr=[354128, 1], y=[1056], period_id=[1], milp_obj=69361.66, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354776], edge_attr=[354776, 1], y=[1056], period_id=[1], milp_obj=75308.544, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353832], edge_attr=[353832, 1], y=[1056], period_id=[1], milp_obj=75566.44, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354668], edge_attr=[354668, 1], y=[1056], period_id=[1], milp_obj=76613.26400000001, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354642], edge_attr=[354642, 1], y=[1056], period_id=[1], milp_obj=82951.23999999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354086], edge_attr=[354086, 1], y=[1056], period_id=[1], milp_obj=97051.508, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354284], edge_attr=[354284, 1], y=[1056], period_id=[1], milp_obj=89437.15599999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354600], edge_attr=[354600, 1], y=[1056], period_id=[1], milp_obj=93457.508, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353728], edge_attr=[353728, 1], y=[1056], period_id=[1], milp_obj=95695.45999999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 356430], edge_attr=[356430, 1], y=[1056], period_id=[1], milp_obj=93458.14799999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 356280], edge_attr=[356280, 1], y=[1056], period_id=[1], milp_obj=88582.052, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354956], edge_attr=[354956, 1], y=[1056], period_id=[1], milp_obj=94055.25999999998, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353592], edge_attr=[353592, 1], y=[1056], period_id=[1], milp_obj=108426.512, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354232], edge_attr=[354232, 1], y=[1056], period_id=[1], milp_obj=120586.46399999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354950], edge_attr=[354950, 1], y=[1056], period_id=[1], milp_obj=126671.04, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353988], edge_attr=[353988, 1], y=[1056], period_id=[1], milp_obj=133773.356, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354144], edge_attr=[354144, 1], y=[1056], period_id=[1], milp_obj=139289.02, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354282], edge_attr=[354282, 1], y=[1056], period_id=[1], milp_obj=158444.272, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354442], edge_attr=[354442, 1], y=[1056], period_id=[1], milp_obj=136464.82, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354650], edge_attr=[354650, 1], y=[1056], period_id=[1], milp_obj=123322.548, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354520], edge_attr=[354520, 1], y=[1056], period_id=[1], milp_obj=138352.956, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353422], edge_attr=[353422, 1], y=[1056], period_id=[1], milp_obj=141753.672, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354718], edge_attr=[354718, 1], y=[1056], period_id=[1], milp_obj=133547.812, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354154], edge_attr=[354154, 1], y=[1056], period_id=[1], milp_obj=119657.60399999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355306], edge_attr=[355306, 1], y=[1056], period_id=[1], milp_obj=111886.53199999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354304], edge_attr=[354304, 1], y=[1056], period_id=[1], milp_obj=129260.58, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355040], edge_attr=[355040, 1], y=[1056], period_id=[1], milp_obj=112656.16399999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354676], edge_attr=[354676, 1], y=[1056], period_id=[1], milp_obj=120868.204, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354580], edge_attr=[354580, 1], y=[1056], period_id=[1], milp_obj=119516.51999999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354290], edge_attr=[354290, 1], y=[1056], period_id=[1], milp_obj=126324.404, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353952], edge_attr=[353952, 1], y=[1056], period_id=[1], milp_obj=131930.88, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355710], edge_attr=[355710, 1], y=[1056], period_id=[1], milp_obj=126289.01999999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355192], edge_attr=[355192, 1], y=[1056], period_id=[1], milp_obj=110078.132, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355052], edge_attr=[355052, 1], y=[1056], period_id=[1], milp_obj=117181.532, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354588], edge_attr=[354588, 1], y=[1056], period_id=[1], milp_obj=97640.044, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355560], edge_attr=[355560, 1], y=[1056], period_id=[1], milp_obj=102678.05200000001, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354420], edge_attr=[354420, 1], y=[1056], period_id=[1], milp_obj=110370.688, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354506], edge_attr=[354506, 1], y=[1056], period_id=[1], milp_obj=115698.24799999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354014], edge_attr=[354014, 1], y=[1056], period_id=[1], milp_obj=122045.58, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355240], edge_attr=[355240, 1], y=[1056], period_id=[1], milp_obj=124632.908, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354014], edge_attr=[354014, 1], y=[1056], period_id=[1], milp_obj=116927.252, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354310], edge_attr=[354310, 1], y=[1056], period_id=[1], milp_obj=123952.68, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354680], edge_attr=[354680, 1], y=[1056], period_id=[1], milp_obj=127979.636, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355052], edge_attr=[355052, 1], y=[1056], period_id=[1], milp_obj=118065.47200000001, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354664], edge_attr=[354664, 1], y=[1056], period_id=[1], milp_obj=120588.74799999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354886], edge_attr=[354886, 1], y=[1056], period_id=[1], milp_obj=112672.22, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355216], edge_attr=[355216, 1], y=[1056], period_id=[1], milp_obj=121728.39199999999, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354960], edge_attr=[354960, 1], y=[1056], period_id=[1], milp_obj=106109.608, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355410], edge_attr=[355410, 1], y=[1056], period_id=[1], milp_obj=111146.18400000001, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353682], edge_attr=[353682, 1], y=[1056], period_id=[1], milp_obj=114271.844, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354628], edge_attr=[354628, 1], y=[1056], period_id=[1], milp_obj=143132.68, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354150], edge_attr=[354150, 1], y=[1056], period_id=[1], milp_obj=142682.472, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354848], edge_attr=[354848, 1], y=[1056], period_id=[1], milp_obj=143122.092, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 355464], edge_attr=[355464, 1], y=[1056], period_id=[1], milp_obj=126971.636, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 354860], edge_attr=[354860, 1], y=[1056], period_id=[1], milp_obj=112854.956, var_names=[1616]),\n",
       " Data(x=[1616, 1616], edge_index=[2, 353284], edge_attr=[353284, 1], y=[1056], period_id=[1], milp_obj=99979.36, var_names=[1616])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd734813-1bbe-4b62-ac52-bb9f5bc2fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdda44d-d9dc-4644-8162-c7e436b9d5be",
   "metadata": {},
   "source": [
    "data_list[0].x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb6d995-3568-4602-be34-90632d008358",
   "metadata": {},
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8c5220-9a21-4d04-a4cd-e4f78151acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_data = pd.read_excel('./32SKU/price_32.xlsx')\n",
    "# target_width = price_data.shape[0]\n",
    "# SKUS = []\n",
    "# for i in range(target_width):\n",
    "#     SKUS.append(price_data.iloc[i][0])\n",
    "# prices = {}\n",
    "# for i in range(target_width):\n",
    "#     prices[SKUS[i]] = price_data.iloc[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f63ec75e-d227-4f67-847e-45420a05fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_array = [1]\n",
    "number = 1\n",
    "loss_array_total = np.zeros(30)\n",
    "predict = np.zeros(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c653ea4-44b9-446e-a23f-65ae6d7a3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'horizon_size':3,\n",
    "        'hidden_size':128,\n",
    "        'quantiles': [0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95],\n",
    "        'columns': ['value'],\n",
    "        'dropout': 0.1,\n",
    "        'b_direction':False,\n",
    "        'lr': 1e-4,\n",
    "        'batch_size': 1,\n",
    "        'num_epochs':10,\n",
    "        'context_size': 16,\n",
    "        'name': number,\n",
    "    }\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#target_width = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3429d8-9d41-42f9-946a-de7144268103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_excel('./32SKU/input_32_half.xlsx', index_col='report_date')\n",
    "#data_df = pd.read_excel('./input_1.xlsx', index_col='report_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b75f68-9541-4ded-9459-18f6de5846f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35780ce3-f16c-4f3b-b87d-5b54dc7739d7",
   "metadata": {},
   "source": [
    "#二分图计算参数准备\n",
    "SKUS = ['EAEZAIN9501', 'EAEZAIN9501EZ3', 'NAEZAIN9501',\n",
    "        'NAEZAIN9501EZ', 'NAEZAIN9501EZ2', 'NAEZAIN9501EZ3',\n",
    "        'NAEZAIN9501EZ4', 'NAEZAIN9502']\n",
    "TOTAL_PERIODS = 184\n",
    "TRAIN_RATIO = 0.8  # 154个训练周期，30个测试周期\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 加载Excel数据\n",
    "#8个sku\n",
    "df_demand = pd.read_excel('demand_0206.xlsx')\n",
    "df_supply = pd.read_excel('supply_0206.xlsx')\n",
    "df_inventory = pd.read_excel('inventory_0206.xlsx')\n",
    "#32个sku\n",
    "# df_demand = pd.read_excel('./32SKU/demand_top32.xlsx')\n",
    "# df_supply = pd.read_excel('./32SKU/supply_top32.xlsx')\n",
    "# df_inventory = pd.read_excel('./32SKU/inventory_top32.xlsx')\n",
    "\n",
    "# 供给量\n",
    "supply = df_supply.set_index('ID').to_dict(orient='index')\n",
    "# 销售量\n",
    "demand = df_demand.set_index('ID').to_dict(orient='index')\n",
    "# 库存日报\n",
    "#initial_inv = df_inventory.set_index('ID').to_dict(orient='index')\n",
    "\n",
    "# 成本参数设置\n",
    "#8个sku\n",
    "prices = {\n",
    "    'EAEZAIN9501': 65, 'EAEZAIN9501EZ3': 70, 'NAEZAIN9501': 69,\n",
    "    'NAEZAIN9501EZ': 84, 'NAEZAIN9501EZ2': 89, 'NAEZAIN9501EZ3': 74,\n",
    "    'NAEZAIN9501EZ4': 79, 'NAEZAIN9502': 74\n",
    "}\n",
    "\n",
    "# 计算各项成本\n",
    "trans_cost = {}\n",
    "for s1 in SKUS:\n",
    "    for s2 in SKUS:\n",
    "        if s1 != s2:\n",
    "            # 改造成本 = 价格差的20%/6\n",
    "            trans_cost[(s1, s2)] = abs(prices[s1] - prices[s2]) * 0.2 / 6\n",
    "\n",
    "holding_cost_factor = 0.068\n",
    "shortage_cost_factor = 0.3\n",
    "# 库存持有成本和缺货成本\n",
    "holding_cost = {s: prices[s] * holding_cost_factor for s in SKUS}\n",
    "shortage_cost = {s: prices[s] * shortage_cost_factor for s in SKUS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3112148-3b3b-43d8-86f8-8d64d3755c8d",
   "metadata": {},
   "source": [
    "# 1. 构建并求解MILP\n",
    "var_values = build_and_solve_milp()\n",
    "# 2. 解析MPS并构建图数据集\n",
    "GNN_dataset = parse_mps_and_build_graphs(var_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a626dfc-d4ff-4da7-bfdb-4f896c25b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GNN_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8e56e60-8b87-4e33-940b-3fcd57ab113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(GNN_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03946bd5-8ebb-46e3-993e-ebb062755843",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNN_dataset = data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e82a071f-ff88-49b1-81c3-3e6e1cae1e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(GNN_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcce37bd-4fb4-4b50-9828-3e2745ac9f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(GNN_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31222d58-6585-472c-a5fc-b5014c962278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_width = 32\n",
    "number = config['name']\n",
    "horizon_size = config['horizon_size'] \n",
    "time_range = pd.DatetimeIndex(data_df.index)\n",
    "#提取target数据\n",
    "series_dict_target = {}\n",
    "#series_dict[0] = data_df['value']\n",
    "i = 0\n",
    "for col in data_df.columns:\n",
    "    if i < ((target_width - 1) * target_width):\n",
    "        series_dict_target[i] = data_df[col]\n",
    "    i += 1\n",
    "    if i > (target_width - 1) * target_width + target_width: break\n",
    "target_df = pd.DataFrame(index = time_range,data=series_dict_target)\n",
    "\n",
    "#提取demand数据\n",
    "series_dict_demand = {}\n",
    "#series_dict[0] = data_df['value']\n",
    "i = 0\n",
    "for col in data_df.columns:\n",
    "    if i >= ((target_width - 1) * target_width) and i < (target_width * target_width):\n",
    "        series_dict_demand[i - ((target_width - 1) * target_width)] = data_df[col]\n",
    "    i+=1\n",
    "    if i >  (target_width + 2) * target_width + 1 : break\n",
    "demand_df = pd.DataFrame(index=time_range,data=series_dict_demand)\n",
    "\n",
    "#提取supply数据\n",
    "series_dict_supply = {}\n",
    "i = 0\n",
    "for col in data_df.columns:\n",
    "    if i >= (target_width * target_width) and i < ((target_width + 1) * target_width):\n",
    "        series_dict_supply[i - (target_width * target_width)] = data_df[col]\n",
    "    i += 1\n",
    "    if i > (target_width + 2) * target_width + 1: break\n",
    "supply_df = pd.DataFrame(index=time_range,data=series_dict_supply)\n",
    "\n",
    "#提取inventory数据\n",
    "series_dict_inventory = {}\n",
    "i = 0\n",
    "for col in data_df.columns:\n",
    "    if i >= ((target_width + 1) * target_width) and i < ((target_width + 2) * target_width):\n",
    "        series_dict_inventory[i - ((target_width + 1) * target_width)] = data_df[col]\n",
    "    i += 1\n",
    "    if i > (target_width + 2) * target_width + 1: break\n",
    "inventory_df = pd.DataFrame(index=time_range,data=series_dict_inventory)\n",
    "\n",
    "#提取特征数据\n",
    "series_dict_covariate = {}\n",
    "i = 0\n",
    "for col in data_df.columns:\n",
    "    if i >= ((target_width + 2) * target_width):\n",
    "        series_dict_covariate[i - ((target_width + 2) * target_width)] = data_df[col]\n",
    "    i += 1\n",
    "covariate_df = pd.DataFrame(index=time_range,data=series_dict_covariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f174c46-68dc-426c-af2a-f8905d7b4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#covariate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e622ca6-4954-43f4-9920-f7bcc3815d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(20 / horizon_size)\n",
    "train_target_df = target_df.iloc[horizon_size:-horizon_size * n]\n",
    "test_target_df = target_df.iloc[-horizon_size * n:]\n",
    "\n",
    "train_demand_df = demand_df.iloc[horizon_size:-horizon_size * n]\n",
    "test_demand_df = demand_df.iloc[-horizon_size * n:]\n",
    "\n",
    "train_supply_df = supply_df.iloc[horizon_size:-horizon_size * n]\n",
    "test_supply_df = supply_df.iloc[-horizon_size * n:]\n",
    "\n",
    "train_inventory_df = inventory_df.iloc[horizon_size:-horizon_size * n]\n",
    "test_inventory_df = inventory_df.iloc[-horizon_size * n:]\n",
    "\n",
    "train_covariate_df = covariate_df.iloc[:-(n + 1) * horizon_size, :]\n",
    "test_covariate_df = covariate_df.iloc[-(n + 1) * horizon_size:-horizon_size, :]\n",
    "\n",
    "train_GNN_dataset = GNN_dataset[:len(train_covariate_df)-horizon_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29ca97bf-b7c2-43d4-8e6f-7a31ba610790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_demand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca1eaabc-c6bd-4538-9c39-fc69cf77a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_supply_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e9c8248-ca38-4e88-846e-6611bbfc57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_inventory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e5f38e3-258f-4810-9ac9-d8cd59af2b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_inventory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c837ace7-a3f5-4151-8fd4-8db3e7fca804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_covariate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6192bf1b-410e-469c-bcf4-3fd72cf00c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#此部分用于生成供蒸馏训练的测试集和训练集\n",
    "test_target_df = target_df\n",
    "test_demand_df = demand_df\n",
    "test_supply_df = supply_df\n",
    "test_inventory_df = inventory_df\n",
    "test_covariate_df = covariate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d67681df-1109-4443-a65d-8e55be311742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 992) (99, 992)\n"
     ]
    }
   ],
   "source": [
    "print(train_target_df.shape,test_target_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1de3e07f-b746-4b34-b674-36b659fee921",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['covariate_size'] = train_covariate_df.shape[1]\n",
    "config['device'] = device\n",
    "config['hidden_size'] = 512\n",
    "config['dropout'] = 0.2\n",
    "config['layer_size'] = 10\n",
    "config['lr'] = 7e-4\n",
    "config['context_size'] = 64\n",
    "config['num_epochs'] = 100\n",
    "horizon_size = config['horizon_size']\n",
    "hidden_size = config['hidden_size']\n",
    "quantiles = config['quantiles']\n",
    "quantiles_size = len(quantiles)\n",
    "columns = config['columns']\n",
    "dropout = config['dropout']\n",
    "layer_size = config['layer_size']\n",
    "b_direction = config['b_direction']\n",
    "lr = config['lr']\n",
    "batch_size= config['batch_size']\n",
    "num_epochs = config['num_epochs']\n",
    "context_size = config['context_size']\n",
    "covariate_size = config['covariate_size']\n",
    "price = list(prices.values())\n",
    "#target_width = 32\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25b635fb-c03c-4069-89a0-89e04ca64a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_ij表示转化关系，第 1 列 A_to_B = 1 表示A可转化为B，第2列 B_to_A = 0表示B不可转化为A\n",
    "a_ij = []\n",
    "for m in range(target_width):\n",
    "    for n in range(target_width):\n",
    "        if n == m:\n",
    "            continue\n",
    "        else:\n",
    "            if price[m] >= price[n]:\n",
    "                a_ij.append(1)\n",
    "            else:a_ij.append(0)\n",
    "n = int(20 / horizon_size)\n",
    "#a_ij = [1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n",
    "#print(a_ij)\n",
    "A = []\n",
    "B = []\n",
    "\n",
    "#约束添加 w_ij + s_ij = M * a_ij (2/11 取消松弛变量，约束为 w_ij - M * a_ij <= 0, 使用relu控制)\n",
    "for i in range(target_width * (target_width - 1)):\n",
    "    trans_restriction = np.zeros(target_width * (target_width - 1))\n",
    "    trans_restriction[i] = 1\n",
    "    #trans_restriction[i + target_width] = 1\n",
    "    #(2/11 取消松弛变量后，A暂时无用)\n",
    "    A.append(trans_restriction)\n",
    "    B.append(a_ij[i]*100000)\n",
    "    \n",
    "A_tensor = torch.tensor(np.array(A))\n",
    "B_tensor = torch.tensor(np.array(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13aff2f6-6dde-4ecf-b81f-50eb32b16eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_covariate_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9720b1d9-86c5-4383-95b2-1ec3bbb4b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_total_loss_average = 0\n",
    "loss_array = np.zeros(n * horizon_size - horizon_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "135c4624-1609-42ec-9010-bc4c7417d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MQRNN(horizon_size, hidden_size, quantiles, columns, dropout, layer_size, b_direction, lr, batch_size,\n",
    "            num_epochs, context_size, covariate_size, target_width, data_list[0].x.shape[0], A_tensor, B_tensor, price, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bd8d7eb-4042-464e-9644-3e1d3e79d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MQRNN_dataset(train_target_df, train_covariate_df, train_demand_df, train_supply_df, train_inventory_df, train_GNN_dataset,\n",
    "                              horizon_size, quantiles_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "500f17bd-f555-4f38-8700-8fca981f4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g = train_dataset[1]\n",
    "a1,b1,c1,d1,e1,f1,g1 = train_dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1757e64d-36ef-490b-86e1-dc618f8ba703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 2017]) torch.Size([75, 3075]) torch.Size([75, 3, 992]) torch.Size([75, 3, 32]) torch.Size([75, 3, 32]) torch.Size([75, 3, 32]) 75\n"
     ]
    }
   ],
   "source": [
    "print(a.size(),b.size(),c.size(),d.size(),e.size(),f.size(),len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e415c5f7-50b7-40b4-843e-a59c62643463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始时间为: 1751542315.2913694\n",
      "epoch_num:0\n",
      "cur_series_tensor.size: torch.Size([1, 75, 2017])\n",
      "GNN_dataset.size: 75\n",
      "Data(x=[1616, 1616], edge_index=[2, 354470], edge_attr=[354470, 1], y=[1056], period_id=[1], milp_obj=173612.596, var_names=[1616])\n",
      "需求矩阵宽度为： torch.Size([1, 75, 3, 32])\n",
      "cur_demand_tensor.size torch.Size([75, 1, 3, 32])\n",
      "GNN输入层格式: 75\n",
      "原LSTM_Encoder输出层格式: torch.Size([75, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yuansheng Si\\Documents\\Zhejiang University\\易点云供需预测_0606\\端到端预测\\真实数据\\真实数据实验\\GNN_LSTM.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n",
      "C:\\Users\\Yuansheng Si\\Documents\\Zhejiang University\\易点云供需预测_0606\\端到端预测\\真实数据\\真实数据实验\\GNN_LSTM.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(edge_index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN_LSTM_Encoder输出层格式: torch.Size([75, 1, 512])\n",
      "demand输出格式为： torch.Size([75, 1, 30])\n",
      "demand输出格式为2： torch.Size([75, 1, 3, 10, 1])\n",
      "torch.Size([75, 1, 3, 10, 1])\n",
      "Ldecoder_output_PJ.size torch.Size([75, 1, 3, 10, 992])\n",
      "Loss is: tensor([-167869.6875], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch_num:1\n",
      "cur_series_tensor.size: torch.Size([1, 75, 2017])\n",
      "GNN_dataset.size: 75\n",
      "Data(x=[1616, 1616], edge_index=[2, 354470], edge_attr=[354470, 1], y=[1056], period_id=[1], milp_obj=173612.596, var_names=[1616])\n",
      "需求矩阵宽度为： torch.Size([1, 75, 3, 32])\n",
      "cur_demand_tensor.size torch.Size([75, 1, 3, 32])\n",
      "GNN输入层格式: 75\n",
      "原LSTM_Encoder输出层格式: torch.Size([75, 1, 512])\n",
      "GNN_LSTM_Encoder输出层格式: torch.Size([75, 1, 512])\n",
      "demand输出格式为： torch.Size([75, 1, 30])\n",
      "demand输出格式为2： torch.Size([75, 1, 3, 10, 1])\n",
      "torch.Size([75, 1, 3, 10, 1])\n",
      "Ldecoder_output_PJ.size torch.Size([75, 1, 3, 10, 992])\n",
      "Loss is: tensor([363289.1875], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch_num:2\n",
      "cur_series_tensor.size: torch.Size([1, 75, 2017])\n",
      "GNN_dataset.size: 75\n",
      "Data(x=[1616, 1616], edge_index=[2, 354470], edge_attr=[354470, 1], y=[1056], period_id=[1], milp_obj=173612.596, var_names=[1616])\n",
      "需求矩阵宽度为： torch.Size([1, 75, 3, 32])\n",
      "cur_demand_tensor.size torch.Size([75, 1, 3, 32])\n",
      "GNN输入层格式: 75\n",
      "原LSTM_Encoder输出层格式: torch.Size([75, 1, 512])\n",
      "GNN_LSTM_Encoder输出层格式: torch.Size([75, 1, 512])\n",
      "demand输出格式为： torch.Size([75, 1, 30])\n",
      "demand输出格式为2： torch.Size([75, 1, 3, 10, 1])\n",
      "torch.Size([75, 1, 3, 10, 1])\n",
      "Ldecoder_output_PJ.size torch.Size([75, 1, 3, 10, 992])\n",
      "Loss is: tensor([6743.6318], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "epoch_num:3\n",
      "cur_series_tensor.size: torch.Size([1, 75, 2017])\n",
      "GNN_dataset.size: 75\n",
      "Data(x=[1616, 1616], edge_index=[2, 354470], edge_attr=[354470, 1], y=[1056], period_id=[1], milp_obj=173612.596, var_names=[1616])\n",
      "需求矩阵宽度为： torch.Size([1, 75, 3, 32])\n",
      "cur_demand_tensor.size torch.Size([75, 1, 3, 32])\n",
      "GNN输入层格式: 75\n",
      "原LSTM_Encoder输出层格式: torch.Size([75, 1, 512])\n",
      "GNN_LSTM_Encoder输出层格式: torch.Size([75, 1, 512])\n",
      "demand输出格式为： torch.Size([75, 1, 30])\n",
      "demand输出格式为2： torch.Size([75, 1, 3, 10, 1])\n",
      "torch.Size([75, 1, 3, 10, 1])\n",
      "Ldecoder_output_PJ.size torch.Size([75, 1, 3, 10, 992])\n",
      "Loss is: tensor([-285109.2500], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始时间为:\u001b[39m\u001b[38;5;124m\"\u001b[39m,start_time)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_GNN_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m已用时:\u001b[39m\u001b[38;5;124m\"\u001b[39m,end_time\u001b[38;5;241m-\u001b[39mstart_time)\n",
      "File \u001b[1;32m~\\Documents\\Zhejiang University\\易点云供需预测_0606\\端到端预测\\真实数据\\真实数据实验\\MQRNN.py:123\u001b[0m, in \u001b[0;36mMQRNN.train\u001b[1;34m(self, dataset, GNN_dataset)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: MQRNN_dataset,GNN_dataset:\u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 123\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgnn_lstm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_lstm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m          \u001b[49m\u001b[43mdemand_forecast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdemand_forecast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m             \u001b[49m\u001b[43mgdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m             \u001b[49m\u001b[43mldecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mldecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m             \u001b[49m\u001b[43mcGNN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcGNN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m             \u001b[49m\u001b[43mGNN_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGNN_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m             \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m             \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m             \u001b[49m\u001b[43mtarget_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m             \u001b[49m\u001b[43mA_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m             \u001b[49m\u001b[43mB_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m             \u001b[49m\u001b[43mprice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m             \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Zhejiang University\\易点云供需预测_0606\\端到端预测\\真实数据\\真实数据实验\\train.py:333\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(encoder, gnn_lstm, demand_forecast, gdecoder, ldecoder, cGNN, dataset, GNN_dataset, lr, batch_size, num_epochs, target_width, A_tensor, B_tensor, price, device)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# 将 L1 正则化项添加到损失函数中\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m#print(\"正则项损失为：\",l1_loss)\u001b[39;00m\n\u001b[0;32m    331\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m lambda_l1 \u001b[38;5;241m*\u001b[39m l1_loss\n\u001b[1;32m--> 333\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    335\u001b[0m gnn_lstm_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\Pytorch-env\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\Pytorch-env\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\Pytorch-env\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"开始时间为:\",start_time)\n",
    "net.train(train_dataset,train_GNN_dataset)\n",
    "end_time = time.time()\n",
    "print(\"已用时:\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ae7e9-a3ab-4991-bae5-6d4daff68148",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_covariate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bfa31-7a96-4ed3-8422-9e0db3717d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"开始时间为:\",start_time)\n",
    "i = 0\n",
    "predict_dict1 = []\n",
    "demand_dict = []\n",
    "#n = int(20 / horizon_size)\n",
    "#知识蒸馏时，需要对训练集数据进行一并预测，所以n为seq总长\n",
    "n = int(test_target_df.shape[0]/horizon_size)\n",
    "print(\"测试集长度为:\",n)\n",
    "for p in range(n * horizon_size - horizon_size + 1):\n",
    "    test_target_df1 = test_target_df[p : p + horizon_size]\n",
    "    test_covariate_df1 = test_covariate_df.iloc[p : p + horizon_size, :]\n",
    "    predict_dict, demand_output = net.predict(train_target_df, train_covariate_df, test_covariate_df1, train_GNN_dataset, target_width, columns, A_tensor, B_tensor)\n",
    "    predict_dict = np.array(list(predict_dict.values()))\n",
    "    demand_output = np.array(list(demand_output.values()))\n",
    "    #逆归一化\n",
    "    # for i in range(quantiles_size):\n",
    "    #     for j in range(horizon_size):\n",
    "    #         for k in range(target_width):\n",
    "    #             predict_dict[i,j,k] = predict_dict[i,j,k]*std[k] + mean[k]\n",
    "    predict_1 = []\n",
    "    demand_1 = []\n",
    "    #取horizon第1天作为预测值\n",
    "    for i in range(quantiles_size):\n",
    "        predict_1.append(predict_dict[i,0])\n",
    "        demand_1.append(demand_output[i,0])\n",
    "    predict_1 = np.array(predict_1)\n",
    "    demand_1 = np.array(demand_1)\n",
    "    #print(\"demand_1:\",demand_1)\n",
    "    predict_2 = []\n",
    "    demand_2 = []\n",
    "    #取10个quantile的平均\n",
    "    for i in range(target_width*(target_width-1)):\n",
    "        #print(\"这是第\",i,\"个决策\")\n",
    "        sum1 = 0\n",
    "        for j in range(quantiles_size):\n",
    "            #print(predict_1[j,i])\n",
    "            sum1 += predict_1[j,i]\n",
    "        predict_2.append(sum1/10)\n",
    "    demand_2.append(sum(demand_1)/10)\n",
    "    predict_dict1.append(predict_2)\n",
    "    demand_dict.append(demand_2)\n",
    "predict_dict1 = pd.DataFrame(predict_dict1)\n",
    "for i in range(len(demand_dict)):\n",
    "    demand_dict[i] = int(demand_dict[i][0])\n",
    "demand_dict = pd.DataFrame(demand_dict)\n",
    "#predict_dict1\n",
    "end_time = time.time()\n",
    "print(\"已用时:\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b3134-3b4e-48de-accb-444fd8cb669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dict1 = predict_dict1.astype(int)\n",
    "predict_dict1.to_excel('output.xlsx', index=True)\n",
    "column_names = data_df.columns[:target_width * (target_width - 1)]\n",
    "predict_dict1.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d857dff-e6f3-4d92-a5f7-faf71c1d9c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demand_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e87517-ba4f-42e4-b467-9b49b878553a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd3158-2cd9-4437-886d-e3ae64050da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(predict_dict1)):\n",
    "#     if(predict_dict1.iloc[i,0] > predict_dict1.iloc[i,1]):\n",
    "#         predict_dict1.iloc[i,0] = int(predict_dict1.iloc[i,0] - predict_dict1.iloc[i,1])\n",
    "#         predict_dict1.iloc[i,1] = 0\n",
    "#     else:\n",
    "#         predict_dict1.iloc[i,1] = int(predict_dict1.iloc[i,1] - predict_dict1.iloc[i,0])\n",
    "#         predict_dict1.iloc[i,0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b214c-2747-4e86-b65d-3ea502049ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_demand_arr = np.array(demand_dict)\n",
    "predict_demand_arr = predict_demand_arr[-18:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16e142-2b5e-4d47-86a1-6953b5e8a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict_demand_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359872e7-1ea6-47db-a393-97da3f5eeb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demand_df_1 = demand_df.iloc[-20:-horizon_size + 1]\n",
    "test_demand_df_1 = test_demand_df_1.sum(axis=1)\n",
    "test_demand_df_1=pd.DataFrame(test_demand_df_1)\n",
    "test_demand_df_arr = np.array(test_demand_df_1)\n",
    "print(len(test_demand_df_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47ef77-0743-4874-9e0b-53f67b995fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demand_df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdb996-6c2b-4085-8000-36a2ae5eda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_demand_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ede2bf-cb2a-4d54-99ac-b027640cf182",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = 0\n",
    "for i in range(len(predict_demand_arr)):\n",
    "    error_rate += 1-abs(predict_demand_arr[i][0]-test_demand_df_arr[i][0])/test_demand_df_arr[i][0]\n",
    "error_rate/=len(predict_demand_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f09ccc-e28d-4ea4-a6a0-e4c293d0a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_loss = loss_result(target_width, predict_dict1, supply_df, inventory_df, demand_df, price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6b1d6-e06a-4e9e-a4f0-f1bbc33fff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学生模型测试集为20天\n",
    "n = int(20 / horizon_size)\n",
    "covariate_size = train_covariate_df.shape[1]\n",
    "predict_dict_student = predict_dict1.iloc[horizon_size :-n * horizon_size, :]\n",
    "train_target_df = train_target_df[:-horizon_size + 1]\n",
    "train_covariate_df = train_covariate_df[:-horizon_size + 1]\n",
    "hidden_size = 1024\n",
    "lr = 7e-4\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d05e0-7d81-4b95-a823-511b263ba086",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_net = Student_Model(hidden_size, lr, batch_size, num_epochs, covariate_size, target_width, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c190bf-c9c4-496d-92bf-405ded345eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"开始时间为:\",start_time)\n",
    "student_net.train(A_tensor,B_tensor,train_target_df,train_covariate_df,predict_dict_student)\n",
    "end_time = time.time()\n",
    "print(\"已用时:\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b6238-3176-4491-9604-9fb12c25822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target_df = target_df.iloc[-horizon_size * n:]\n",
    "test_covariate_df = covariate_df.iloc[-(n + 1) * horizon_size:-horizon_size, :]\n",
    "print(test_covariate_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e94bea-e65b-4c17-83a7-b6e5376b8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_target_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4ff99-3a55-4b33-866c-20400d056d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"开始时间为:\",start_time)\n",
    "student_prediction, student_loss = student_net.predict(A_tensor,B_tensor,test_target_df,test_covariate_df)\n",
    "end_time = time.time()\n",
    "print(\"已用时:\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a319ae-4ed2-44bb-9739-3ab9065a1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_prediction = pd.DataFrame(student_prediction)\n",
    "student_prediction = student_prediction.astype(int)\n",
    "student_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1eaae-e00a-454d-987f-ee21f4833321",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_loss = loss_result(target_width, student_prediction, supply_df.iloc[-n*horizon_size:,], inventory_df.iloc[-n*horizon_size:,], demand_df.iloc[-n*horizon_size:,], price)\n",
    "print(student_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78634d59-f562-469b-a5c6-226ab142943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gurobi_dict = data_df.iloc[:-horizon_size + 1,(target_width + 2) * target_width + 2:(2 * target_width + 1) * target_width + 2]\n",
    "Gurobi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cc600-23de-48f8-ae11-d9e05194df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gurobi_loss = loss_result(target_width, Gurobi_dict, supply_df, inventory_df, demand_df, price)\n",
    "print(Gurobi_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced46b27-2525-487e-9f02-7ec63f869457",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"需求预测准确率为：\",error_rate*100,\"%\")\n",
    "print(\"求解器loss为：\",Gurobi_loss)\n",
    "print(\"老师模型loss为：\",teacher_loss)\n",
    "print(\"学生模型loss为：\",student_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
